{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"ML_Task_BaiduOutput.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hSUs-0sOy3Hu","colab_type":"code","outputId":"31c88db8-e930-4052-c4de-ca00ec3aed30","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GUKmkzV7y7qA","colab_type":"code","colab":{}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wp-GyXC9y7we","colab_type":"code","colab":{}},"source":["from drive.ML_Study import * # 让driver连接云盘的ML_Study"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iBmuDnSr2UpP","colab_type":"code","colab":{}},"source":["function ClickConnect(){\n","  console.log(\"Working\"); \n","  document\n","    .querySelector(\"#top-toolbar > colab-connect-button\")\n","    .shadowRoot\n","    .querySelector(\"#connect\")\n","    .click()\n","}\n"," \n","setInterval(ClickConnect,60000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wmg_wOqhy4ga","colab_type":"code","outputId":"20c10021-5bab-4676-bd9c-45c6b0dedb4c","executionInfo":{"status":"ok","timestamp":1589357680698,"user_tz":-480,"elapsed":18860,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":574}},"source":["pip install transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n","\r\u001b[K     |▌                               | 10kB 22.8MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 29.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 27.7MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 21.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 14.5MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 13.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 12.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 112kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 225kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 307kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 368kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 389kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 419kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 450kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 481kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 501kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 532kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 563kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 583kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 593kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 614kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 12.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 48.5MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 42.6MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 38.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=fd7a9f6fc3b7d900a06c4b5afde4510b7487eec3ef2382dcf011a11fb73bd280\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9n-0XdJqy4ow","colab_type":"code","colab":{}},"source":["import json\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","import tensorflow as tf\n","from transformers import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_1A-6KZLy3Hx","colab_type":"code","colab":{}},"source":["maxlen=128"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wfdLQkxwy3Hz","colab_type":"code","outputId":"a2bc316d-95fd-442e-fd40-a8df4c843781","executionInfo":{"status":"ok","timestamp":1589357689372,"user_tz":-480,"elapsed":11710,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["print('Lodaing BERT')\n","pretrainedModel_path = 'drive/ML_Study/bert-base-chinese'\n","tokenizer = BertTokenizer.from_pretrained(os.path.join(pretrainedModel_path, 'vocab.txt'))\n","config = BertConfig.from_json_file(os.path.join(pretrainedModel_path, 'config.json'))\n","print('==========END==========')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Lodaing BERT\n"],"name":"stdout"},{"output_type":"stream","text":["Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"],"name":"stderr"},{"output_type":"stream","text":["==========END==========\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8vqN_D42y3H4","colab_type":"code","colab":{}},"source":["# 生成新的SPO\n","def load_data(path):\n","    D = []\n","    with open(path, encoding='utf-8') as f:\n","        for l in f.readlines():\n","            l = json.loads(l)\n","            D.append(\n","             l['text']\n","            )\n","    return D\n","\n","# 得到s,p对应值的初始下标\n","def search(pattern, sequence):\n","    n = len(pattern)\n","    for i in range(len(sequence)):\n","        if sequence[i:i + n] == pattern:\n","            return i\n","    return -1\n","\n","# 寻找特殊符号的位置\n","def search_special_label(pattern, sequence):\n","    rt = []\n","    for i, sid in enumerate(sequence):\n","        if sid == pattern:\n","            rt.append(i)\n","    return rt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i1wlwSE9y3H6","colab_type":"code","colab":{}},"source":["# category mapping\n","def get_mapping(mapping_path):\n","    predicate2id, id2predicate = {}, {}\n","    with open(mapping_path, encoding='utf-8') as f:\n","        for l in f.readlines():\n","            l = json.loads(l)\n","            for key in l['object_type']:\n","                p = (l['predicate']+'_'+key)\n","                if p not in predicate2id:\n","                    id2predicate[len(predicate2id)] = p\n","                    predicate2id[p] = len(predicate2id)\n","    return predicate2id, id2predicate\n","    \n","mapping_path = 'drive/ML_Study/Baidu_ERE_Data/schema.json'\n","\n","predicate2id, id2predicate = get_mapping(mapping_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W6EfQXPRy3H8","colab_type":"code","colab":{}},"source":["# get BIO-relation label mapping\n","# get predicate mapping\n","special_labels = ['[PAD]','[CLS]','[SEP]','[UNK]']\n","special_labels_token = [0,101,102,100]\n","# All label\n","labels = []\n","for l in predicate2id.keys():\n","    l = 'B-Sub-'+l\n","    labels.append(l)\n","for l in predicate2id.keys():\n","    l = 'B-Obj-'+l\n","    labels.append(l)\n","labels.append('I')\n","labels.append('O')\n","for l in special_labels:\n","    labels.append(l)\n","    \n","# Bilabel Mapping\n","id2BIO = {}\n","BIO2id = {}\n","for i,label in enumerate(labels):\n","    id2BIO[i] = label\n","    BIO2id[label] = i"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hrBNX9yLy3H-","colab_type":"code","colab":{}},"source":["def get_input(data,tokenizer,maxlen=128):\n","    token_ids, segment_ids, position_ids= [], [], []\n","    for t in data:\n","        token_id, segment_id, position_id = tokenizer.encode_plus(t, max_length=maxlen,pad_to_max_length=True).values()\n","        token_ids.append(token_id)\n","        segment_ids.append(segment_id)\n","        position_ids.append(position_id)\n","    return [token_ids,position_ids,segment_ids]\n","#     return {'input_ids':tf.convert_to_tensor(tf.constant(token_ids)),\n","#             'attention_mask':tf.convert_to_tensor(tf.constant(position_ids)),\n","#             'token_type_ids':tf.convert_to_tensor(tf.constant(segment_ids))}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xMhfxiYIy3IA","colab_type":"code","colab":{}},"source":["# test_data = load_data('drive/ML_Study/Baidu_ERE_Data/test1_data.json')\n","test_data = load_data('drive/ML_Study/Baidu_ERE_Data/test2_data.json')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xp4vYF-my3IC","colab_type":"code","outputId":"98dec372-ece1-4e80-b2e8-e5e9905fc3fb","executionInfo":{"status":"ok","timestamp":1589357770865,"user_tz":-480,"elapsed":68763,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print('Contructing Data')\n","# train_xy = get_input(train_data,tokenizer,predicate2id,maxlen=128)\n","test_input = get_input(test_data,tokenizer,maxlen=maxlen)\n","test_dataset = {'input_ids':tf.convert_to_tensor(tf.constant(test_input[0])),\n","            'attention_mask':tf.convert_to_tensor(tf.constant(test_input[1])),\n","            'token_type_ids':tf.convert_to_tensor(tf.constant(test_input[2]))}\n","print('==========END==========')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Contructing Data\n","==========END==========\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T9VZl3F_y3IE","colab_type":"code","colab":{}},"source":["class SPO_Model(TFBertPreTrainedModel):\n","    def __init__(self, config, *inputs, **kwargs):\n","        super().__init__(config, *inputs, **kwargs)\n","        self.bert = TFBertMainLayer(config, name=\"bert\")\n","        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","        self.BiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences=True))\n","        self.classifier = tf.keras.layers.Dense(116,activation=tf.nn.sigmoid,kernel_initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05))\n","    def call(self, inputs, **kwargs):\n","        bert_outputs = self.bert(inputs, **kwargs)\n","        sequence_output = bert_outputs[0]\n","        x = self.dropout(sequence_output, training=kwargs.get(\"training\", False))\n","        x = self.BiLSTM(x)\n","        x = self.classifier(x)\n","        x = tf.keras.layers.Lambda(lambda x: x**(1/0.95))(x) #对BinaryEntropy进行放大处理，以排除人工标注错误，假设95%的数据都是正确的\n","\n","        # CRF Layer 其实不适合该模型\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzEka-fAy3IH","colab_type":"code","colab":{}},"source":["spo_model = SPO_Model.from_pretrained(pretrainedModel_path,config=config)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oSg94Nk1y3IK","colab_type":"code","outputId":"a222cd44-7e58-4b1d-c13e-ab793ea30123","executionInfo":{"status":"ok","timestamp":1589357808765,"user_tz":-480,"elapsed":101817,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["spo_model.load_weights('drive/ML_Study/saved_Model/weight_200410v4/spo_model_20epochs.ckpt')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb350b8b048>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"tI87YBjiVxPv","colab_type":"code","colab":{}},"source":["# m = SPO_Model.from_pretrained(pretrainedModel_path,config=config)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4-YSmhQVzz-","colab_type":"code","colab":{}},"source":["# m.load_weights('drive/ML_Study/saved_Model/weight_200410v3/spo_model_15epochs.ckpt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XtR2Kx2y3IM","colab_type":"code","colab":{}},"source":["# label -> SPOes\n","def predict_SPOes(token_id, label): # zip(token_id, label_pred)\n","  SPOes=[]\n","  sub_text_p, sub_category_p = np.where(label[:,:55]>0.5) \n","  obj_text_p, obj_category_p = np.where(label[:,55:110]>0.5) # obj_category_p 自动减去48!! 返回的值是从 0 -> 47!\n","  i_text_p = sorted(np.where(label[:,110]>0.5)) # I 的对应下标\n","  sub_info = list(zip(sub_text_p, sub_category_p))\n","  obj_info = list(zip(obj_text_p, obj_category_p))\n","\n","  SPOes=[]\n","\n","  for sub in sub_info:\n","    sub_start = sub[0]\n","    sub_end = sub_start\n","    for I_i in i_text_p[0]: # 确定 sub 的长度\n","        if I_i == (sub_end + 1):\n","            sub_end += 1\n","    for obj in obj_info: \n","        if sub[1] == (obj[1]):\n","            obj_start = obj[0]\n","            obj_end =  obj_start\n","            for I_i in i_text_p[0]: # 确定 obj 的长度\n","                if I_i == (obj_end + 1):\n","                    obj_end += 1\n","            spo = (\n","                (tokenizer.decode(token_id[sub_start: sub_end+1])).replace(\" \",\"\"),\n","                id2predicate[sub[1]],\n","                (tokenizer.decode(token_id[obj_start: obj_end+1])).replace(\" \",\"\")\n","                )\n","            SPOes.append(spo)\n","  return SPOes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRMIypAwy3IN","colab_type":"code","colab":{}},"source":["def get_relation_pattern(mapping_path):\n","    relation_pattern = {}\n","    with open(mapping_path,encoding='utf-8') as f:\n","        for l in f.readlines():\n","            l = json.loads(l)\n","            relation_pattern[l['predicate']] = (l['object_type'],l['subject_type'])\n","    return relation_pattern\n","\n","relation_pattern = get_relation_pattern(mapping_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LLTv52S7y3IP","colab_type":"code","colab":{}},"source":["# SPOes -> final output\n","def get_spo_list(SPOes,relation_pattern):\n","    complex_relations = ['配音','上映时间','票房','获奖','饰演']\n","    spo_list= []\n","    for i1, spo in enumerate(SPOes):\n","        p = spo[1].split('_') # 切开predicate\n","        if p[1]  == '@value':\n","            obj={}\n","            obj[p[1]] = spo[2]\n","            obj_type = {}\n","            obj_type[p[1]] = relation_pattern[p[0]][0][p[1]] # object_type\n","            if p[0] in complex_relations: # 如果是复杂关系就找其子预测       \n","                for i2, spo_c in enumerate(SPOes):\n","                    if i1 == i2:\n","                        continue\n","                    p_c = spo_c[1].split('_')\n","                    if p_c[0] == p[0]:\n","                        obj[p_c[1]] = spo_c[2]\n","                        obj_type[p_c[1]] = relation_pattern[p[0]][0][p_c[1]] # object_type\n","        \n","            spo_list.append({\n","                        'predicate':p[0],\n","                        'object_type':obj_type,# 只保留存在的key\n","                        'subject_type':relation_pattern[p[0]][1],\n","                        'object':obj,\n","                        'subject':spo[0]\n","            })\n","    return spo_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wUz0s_whXCnT","colab_type":"code","colab":{}},"source":["rt_labels  = []\n","\n","for i in test_input[0]:\n","  rt_labels.append(spo_model(tf.constant([i]))[0])\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TdewpIO_y3IT","colab_type":"code","colab":{}},"source":["f = open('drive/test2_pred.json', 'w', encoding='utf-8')\n","\n","for text, token_id, label in zip(test_data,test_input[0], rt_labels):\n","    SPOes = predict_SPOes(token_id, label.numpy())\n","    out = json.dumps({\n","        'text': text,\n","        'spo_list':get_spo_list(SPOes,relation_pattern)\n","    },ensure_ascii=False)\n","    f.write(out + '\\n')\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZM2HUnToYK44","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0r1EWPGxYK73","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T5oRFB-LTZgB","colab_type":"code","colab":{}},"source":["def get_input(data,tokenizer,predicate2id,maxlen=128):\n","    special_labels_token = [0,101,102,100]\n","    token_ids, segment_ids, position_ids= [], [], []\n","    spo_labels = []\n","    for d in data:\n","        # get X \n","        token_id, segment_id, position_id = tokenizer.encode_plus(d['text'], max_length=maxlen,pad_to_max_length=True).values()\n","        token_ids.append(token_id)\n","        segment_ids.append(segment_id)\n","        position_ids.append(position_id)\n","        # get Y\n","        # 116 = 2R + 4 + 2, R=55\n","        # I:110, O:111\n","        label = np.zeros((116,maxlen)) # len(labels), maxlen\n","        label[111,:] = 1 # 先设置所有标签都是 O\n","        for s, p, o in d['spo_list']:\n","            s = tokenizer.encode(s)[1:-1]# 101 是 CLS 102 是ESP 这里只是为了得到s内部值的ID\n","            p_i = predicate2id[p] # 对category进行映射得到对应的ID\n","            o = tokenizer.encode(o)[1:-1] \n","            s_i = search(s, token_id) # 对text进行映射\n","            o_i = search(o, token_id)\n","            # Sub\n","            label[p_i,s_i] = 1\n","            label[111,s_i] = 0\n","            for i in range(1,len(s)):\n","                s_I_i = s_i + i\n","                label[110,s_I_i] = 1\n","                label[111,s_I_i] = 0\n","            # Obj\n","            label[55+p_i,o_i] = 1\n","            label[111,o_i] = 0\n","            for i in range(1,len(o)):\n","                o_I_i = o_i + i\n","                label[110,o_I_i] = 1\n","                label[111,o_I_i] = 0\n","        # specail label\n","        for i, sp in enumerate(special_labels_token):\n","            sp_is = search_special_label(sp, token_id)\n","            for sp_i in sp_is:\n","                label[112+i,sp_i] = 1\n","                label[111,sp_i] = 0\n","                \n","        spo_labels.append(label.T)\n","    # return [\n","    #         tf.convert_to_tensor(tf.constant(token_ids)),\n","    #         tf.convert_to_tensor(tf.constant(position_ids)),\n","    #         tf.convert_to_tensor(tf.constant(segment_ids)),\n","    #         tf.convert_to_tensor(tf.constant(spo_labels))\n","    # ]\n","    return [token_ids, position_ids, segment_ids, spo_labels] "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DbEwMAGCTi_y","colab_type":"code","colab":{}},"source":["def load_datav2(path):\n","    D = []\n","    with open(path, encoding='utf-8') as f:\n","        for l in f.readlines():\n","            l = json.loads(l)\n","            D.append({\n","                'text': l['text'],\n","                'spo_list': [\n","                    (spo['subject'], (spo['predicate']+'_'+key), spo['object'][key])\n","                    for spo in l['spo_list'] for key in spo['object'] \n","                ]\n","            })\n","    return D"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqR1xLMITZjE","colab_type":"code","colab":{}},"source":["sample_train_data = load_datav2('drive/ML_Study/Baidu_ERE_Data/sample_data.json')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mne0zhY_TZmD","colab_type":"code","colab":{}},"source":["sample_train_xy = get_input(sample_train_data,tokenizer,predicate2id,maxlen=128)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QH1SOlLTTo-W","colab_type":"code","outputId":"b3cf3bb9-fe3c-4e7a-9d5d-742595b3692c","executionInfo":{"status":"ok","timestamp":1586610215983,"user_tz":-480,"elapsed":1491,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["a = sample_train_data[18]['text']\n","b = sample_train_data[18]['spo_list']\n","print(a)\n","print(b)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1990年2月，苏有朋和陈志朋、吴奇隆联袂出演了电影《好小子之游侠儿》，在影片中扮演了一个富有科技头脑，又带有稚气的高中生“小乖”，并合唱影片主题曲《游侠儿》，本片获得1990年度台湾“十大最高票房电影”之一\n","[('苏有朋', '获奖_onDate', '1990年'), ('苏有朋', '获奖_@value', '十大最高票房电影')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UbP9mTI7WT7s","colab_type":"code","colab":{}},"source":["rt = predict_SPOes(a_token, label[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"akB3W7S0Tpdh","colab_type":"code","outputId":"02411307-0cad-4862-8bee-f981459c7ae8","executionInfo":{"status":"ok","timestamp":1586610218983,"user_tz":-480,"elapsed":1140,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["a_token = tokenizer.encode(a, max_length=128, pad_to_max_length=True)\n","print(a_token)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[101, 8431, 2399, 123, 3299, 8024, 5722, 3300, 3301, 1469, 7357, 2562, 3301, 510, 1426, 1936, 7384, 5468, 6146, 1139, 4028, 749, 4510, 2512, 517, 1962, 2207, 2094, 722, 3952, 899, 1036, 518, 8024, 1762, 2512, 4275, 704, 2815, 4028, 749, 671, 702, 2168, 3300, 4906, 2825, 1928, 5554, 8024, 1348, 2372, 3300, 4928, 3698, 4638, 7770, 704, 4495, 100, 2207, 731, 100, 8024, 2400, 1394, 1548, 2512, 4275, 712, 7579, 3289, 517, 3952, 899, 1036, 518, 8024, 3315, 4275, 5815, 2533, 8431, 2399, 2428, 1378, 3968, 100, 1282, 1920, 3297, 7770, 4873, 2791, 4510, 2512, 100, 722, 671, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w1DaOgWTTwPy","colab_type":"code","colab":{}},"source":["label = spo_model(tf.constant([a_token]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pfOwg3UST1cK","colab_type":"code","outputId":"7de0a576-6425-4b31-f291-83eae9b4bc64","executionInfo":{"status":"ok","timestamp":1586610374028,"user_tz":-480,"elapsed":1127,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["get_spo_list(rt,relation_pattern)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'object': {'@value': '十 大 最 高 票 房 电 影 [UNK]', 'onDate': '1990'},\n","  'object_type': {'@value': '奖项', 'onDate': 'Date'},\n","  'predicate': '获奖',\n","  'subject': '苏 有 朋',\n","  'subject_type': '娱乐人物'},\n"," {'object': {'@value': '台 湾 [UNK] 十 大 最 高 票 房 电 影 [UNK]', 'onDate': '1990'},\n","  'object_type': {'@value': '奖项', 'onDate': 'Date'},\n","  'predicate': '获奖',\n","  'subject': '苏 有 朋',\n","  'subject_type': '娱乐人物'}]"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"code","metadata":{"id":"ioWD9bmXT-tX","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}