{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"ML_Task_Baidu04_colab.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rdDYEOMUmAvN","colab_type":"code","outputId":"75b2ce93-89ae-43d5-fa43-dfe39dac45a9","executionInfo":{"status":"ok","timestamp":1587734133909,"user_tz":-480,"elapsed":63010,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":222}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 144568 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.21-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.21-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.21-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TaU3yOQ5mEQc","colab_type":"code","colab":{}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTNTGW6MmJVV","colab_type":"code","colab":{}},"source":["from drive.ML_Study import * # 让driver连接云盘的ML_Study"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yqr5uYVhmMiI","colab_type":"code","colab":{}},"source":["# For console\n","# 防断连：f12 复制到console里边运行 \n","function ClickConnect(){\n","  console.log(\"Working\"); \n","  document\n","    .querySelector(\"#top-toolbar > colab-connect-button\")\n","    .shadowRoot\n","    .querySelector(\"#connect\")\n","    .click()\n","}\n"," \n","setInterval(ClickConnect,60000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RxTB8oaVmkQI","colab_type":"code","outputId":"63ca6253-df6c-44f9-8b36-89a11ad00ef0","executionInfo":{"status":"ok","timestamp":1587734176298,"user_tz":-480,"elapsed":9226,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":675}},"source":["pip install transformers"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\u001b[K     |████████████████████████████████| 573kB 2.7MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.40)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 51.7MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 43.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 51.8MB/s \n","\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.40)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (2.8.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=3c77a282629598fd2ba0cc378bcf319bd0767a2db4f404a1fba137a3058591e7\n","  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RbVOUGI2mEdD","colab_type":"code","colab":{}},"source":["import json\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","import tensorflow as tf\n","from transformers import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZXIiuQzn93S","colab_type":"code","colab":{}},"source":["# input最大长度\n","maxlen=128"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Vff5fc0y30f","colab_type":"code","colab":{}},"source":["# if 'COLAB_TPU_ADDR' not in os.environ:\n","#     print('ERROR: Not connected to a TPU runtime')\n","# else:\n","#     tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","#     print ('TPU address is', tpu_address)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VmmRkNpqq0hb","colab_type":"code","colab":{}},"source":["# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","# tf.config.experimental_connect_to_cluster(resolver)\n","# tf.tpu.experimental.initialize_tpu_system(resolver)\n","# strategy = tf.distribute.experimental.TPUStrategy(resolver)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IfklaHtBmAvQ","colab_type":"code","outputId":"2b400a36-4aa1-4cca-b89c-f2d109e22c89","executionInfo":{"status":"ok","timestamp":1587734189223,"user_tz":-480,"elapsed":5541,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["print('Lodaing BERT')\n","pretrainedModel_path = 'drive/ML_Study/bert-base-chinese'\n","tokenizer = BertTokenizer.from_pretrained(os.path.join(pretrainedModel_path, 'vocab.txt'))\n","config = BertConfig.from_json_file(os.path.join(pretrainedModel_path, 'config.json'))\n","print('==========END==========')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Lodaing BERT\n"],"name":"stdout"},{"output_type":"stream","text":["Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"],"name":"stderr"},{"output_type":"stream","text":["==========END==========\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VqAK1c6DmAvU","colab_type":"code","colab":{}},"source":["# 生成新的SPO\n","def load_data(path):\n","    D = []\n","    with open(path, encoding='utf-8') as f:\n","        for l in f.readlines():\n","            l = json.loads(l)\n","            D.append({\n","                'text': l['text'],\n","                'spo_list': [\n","                    (spo['subject'], (spo['predicate']+'_'+key), spo['object'][key])\n","                    for spo in l['spo_list'] for key in spo['object'] \n","                ]\n","            })\n","    return D\n","\n","# 得到s,p对应值的初始下标\n","def search(pattern, sequence):\n","    n = len(pattern)\n","    for i in range(len(sequence)):\n","        if sequence[i:i + n] == pattern:\n","            return i\n","    return -1\n","\n","# 寻找特殊符号的位置\n","def search_special_label(pattern, sequence):\n","    rt = []\n","    for i, sid in enumerate(sequence):\n","        if sid == pattern:\n","            rt.append(i)\n","    return rt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"APw3KlkzmAvW","colab_type":"code","outputId":"52fb65c3-184b-453b-c914-1df20de7b8fb","executionInfo":{"status":"ok","timestamp":1587734190932,"user_tz":-480,"elapsed":6149,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print('Loading Data')\n","# sample_train_data = load_data('drive/ML_Study/Baidu_ERE_Data/sample_data.json')\n","# train_data = load_data('drive/ML_Study/Baidu_ERE_Data/train_data.json')\n","valid_data = load_data('drive/ML_Study/Baidu_ERE_Data/dev_data.json')\n","# test_data = load_data('./data/example.test')\n","print('=========END==========')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Loading Data\n","=========END==========\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aohGFyR7mAvY","colab_type":"code","colab":{}},"source":["# category mapping\n","def get_mapping(mapping_path):\n","    predicate2id, id2predicate = {}, {}\n","    with open(mapping_path, encoding='utf-8') as f:\n","        for l in f.readlines():\n","            l = json.loads(l)\n","            for key in l['object_type']:\n","                p = (l['predicate']+'_'+key)\n","                if p not in predicate2id:\n","                    id2predicate[len(predicate2id)] = p\n","                    predicate2id[p] = len(predicate2id)\n","    return predicate2id, id2predicate\n","    \n","mapping_path = 'drive/ML_Study/Baidu_ERE_Data/schema.json'\n","\n","predicate2id, id2predicate = get_mapping(mapping_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSeCNf80mAvb","colab_type":"code","colab":{}},"source":["# get BIO-relation label mapping\n","# get predicate mapping\n","special_labels = ['[PAD]','[CLS]','[SEP]','[UNK]']\n","special_labels_token = [0,101,102,100]\n","# All label\n","labels = []\n","for l in predicate2id.keys():\n","    l = 'B-Sub-'+l\n","    labels.append(l)\n","for l in predicate2id.keys():\n","    l = 'B-Obj-'+l\n","    labels.append(l)\n","labels.append('I')\n","labels.append('O')\n","for l in special_labels:\n","    labels.append(l)\n","    \n","# Bilabel Mapping\n","id2BIO = {}\n","BIO2id = {}\n","for i,label in enumerate(labels):\n","    id2BIO[i] = label\n","    BIO2id[label] = i"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yeBjbqlKmAvq","colab_type":"code","colab":{}},"source":["# 这个func应该没用了\n","\n","# 0 1 形式label转为 BIO 形式 \n","# 由于可能一个实体可能既是SUB又是OBJ 而且可能对应多种关系 故采用list的格式存储\n","def visualized_label(label,maxlen=128):\n","    visual_label =[]\n","    for i in range(maxlen):\n","        a=[]\n","        visual_label.append(a)\n","    text_p, category_p= np.where(label>0.5)\n","    for p in zip(text_p, category_p):\n","        visual_label[p[0]].append(id2BIO[p[1]])\n","    return visual_label\n","\n","# print(visualized_abel(label))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFiz0UU2mAvs","colab_type":"code","colab":{}},"source":["# spo 转为 X 和 0 1 形式的 label Y\n","# label: dimensionX->sequencePosition, dimensionY->CategoryPosition\n","def get_input(data,tokenizer,predicate2id,maxlen=128):\n","    special_labels_token = [0,101,102,100]\n","    token_ids, segment_ids, position_ids= [], [], []\n","    spo_labels = []\n","    for d in data:\n","        # get X \n","        token_id, segment_id, position_id = tokenizer.encode_plus(d['text'], max_length=maxlen,pad_to_max_length=True).values()\n","        token_ids.append(token_id)\n","        segment_ids.append(segment_id)\n","        position_ids.append(position_id)\n","        # get Y\n","        # 116 = 2R + 4 + 2, R=55\n","        # I:110, O:111\n","        label = np.zeros((116,maxlen)) # len(labels), maxlen\n","        label[111,:] = 1 # 先设置所有标签都是 O\n","        for s, p, o in d['spo_list']:\n","            s = tokenizer.encode(s)[1:-1]# 101 是 CLS 102 是ESP 这里只是为了得到s内部值的ID\n","            p_i = predicate2id[p] # 对category进行映射得到对应的ID\n","            o = tokenizer.encode(o)[1:-1] \n","            s_i = search(s, token_id) # 对text进行映射\n","            o_i = search(o, token_id)\n","            # Sub\n","            label[p_i,s_i] = 1\n","            label[111,s_i] = 0\n","            for i in range(1,len(s)):\n","                s_I_i = s_i + i\n","                label[110,s_I_i] = 1\n","                label[111,s_I_i] = 0\n","            # Obj\n","            label[55+p_i,o_i] = 1\n","            label[111,o_i] = 0\n","            for i in range(1,len(o)):\n","                o_I_i = o_i + i\n","                label[110,o_I_i] = 1\n","                label[111,o_I_i] = 0\n","        # specail label\n","        for i, sp in enumerate(special_labels_token):\n","            sp_is = search_special_label(sp, token_id)\n","            for sp_i in sp_is:\n","                label[112+i,sp_i] = 1\n","                label[111,sp_i] = 0\n","                \n","        spo_labels.append(label.T)\n","    # return [\n","    #         tf.convert_to_tensor(tf.constant(token_ids)),\n","    #         tf.convert_to_tensor(tf.constant(position_ids)),\n","    #         tf.convert_to_tensor(tf.constant(segment_ids)),\n","    #         tf.convert_to_tensor(tf.constant(spo_labels))\n","    # ]\n","    return [token_ids, position_ids, segment_ids, spo_labels] \n","    # return [{'input_ids':token_ids, 'attention_mask':position_ids, 'token_type_ids':segment_ids},spo_labels]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTi9uxhPmAvu","colab_type":"code","outputId":"2aad79d9-60c7-435a-9fbe-f74d918f63b1","executionInfo":{"status":"ok","timestamp":1587734213907,"user_tz":-480,"elapsed":26165,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print('Contructing Data')\n","# sample_train_xy = get_input(sample_train_data,tokenizer,predicate2id,maxlen=128)\n","# train_xy = get_input(train_data,tokenizer,predicate2id,maxlen=128)\n","valid_xy = get_input(valid_data,tokenizer,predicate2id,maxlen=128)\n","print('==========END==========')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Contructing Data\n","==========END==========\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OOd5f92k4nM7","colab_type":"code","colab":{}},"source":["# train_dataset = tf.data.Dataset.from_tensor_slices((train_xy[0], train_xy[1])) # 构建 tf.dataset\n","# train_dataset = train_dataset.shuffle(buffer_size=1000)\n","# train_dataset = train_dataset.batch(batch_size=16)\n","# train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLPzfa4xJO7n","colab_type":"code","colab":{}},"source":["tfr_data_path_sample = r'drive/ML_Study/Baidu_ERE_Data/TFrecord_data/sample_train.tfrecords'\n","tfr_data_path_train = r'drive/ML_Study/Baidu_ERE_Data/TFrecord_data/train.tfrecords'\n","tfr_data_path_dev = r'drive/ML_Study/Baidu_ERE_Data/TFrecord_data/valid.tfrecords' "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j92O929mNTA4","colab_type":"code","colab":{}},"source":["# 转为tfrecords \n","def generate_tfr(path,data):\n","  with tf.io.TFRecordWriter(tfr_data_path_dev) as writer:\n","    i = 0\n","    for token_id, position_id, segment_id, spo_label in zip(data[0],data[1],data[2],data[3]):\n","        i+=1\n","        if i%5000 == 0:\n","          print('The '+str(i)+'th examples')\n","        feature ={\n","            'input_ids' : tf.train.Feature(int64_list=tf.train.Int64List(value=token_id)),\n","            'attention_mask' : tf.train.Feature(int64_list=tf.train.Int64List(value=position_id)),\n","            'token_type_ids' : tf.train.Feature(int64_list=tf.train.Int64List(value=segment_id)),\n","            'label' : tf.train.Feature(float_list=tf.train.FloatList(value=spo_label.flatten())) # 2D numpy 处理\n","        }\n","        example = tf.train.Example(features=tf.train.Features(feature=feature))\n","        writer.write(example.SerializeToString())  \n","    print('==========END==========')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpZNy9ipOFfL","colab_type":"code","colab":{}},"source":["# generate_tfr(tfr_data_path_sample,sample_train_xy)\n","# generate_tfr(tfr_data_path,train_xy)\n","# generate_tfr(tfr_data_path_dev,valid_xy)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7NGTyPNGFyhb","colab_type":"code","colab":{}},"source":["# decode\n","# sample_raw_dataset = tf.data.TFRecordDataset(tfr_data_path_sample) \n","train_raw_dataset = tf.data.TFRecordDataset(tfr_data_path_train) \n","valid_raw_dataset = tf.data.TFRecordDataset(tfr_data_path_dev)\n","\n","feature_description = { # 定义Feature结构，告诉解码器每个Feature的类型是什么\n","    'input_ids' : tf.io.FixedLenFeature([128], tf.int64),\n","    'attention_mask' : tf.io.FixedLenFeature([128], tf.int64),\n","    'token_type_ids' : tf.io.FixedLenFeature([128], tf.int64),\n","    'label' : tf.io.FixedLenFeature((128,116), tf.float32)\n","}\n","\n","def _read_and_decode(example_string): # 将 TFRecord 文件中的每一个序列化的 tf.train.Example 解码\n","    feature_dict = tf.io.parse_single_example(example_string, feature_description)\n","    # token_ids = tf.cast(feature_dict['input_ids'], tf.int64)\n","    # attention_mask = tf.cast(feature_dict['attention_mask'], tf.int64)\n","    # token_type_ids = tf.cast(feature_dict['token_type_ids'], tf.int64)\n","    # label = tf.cast(feature_dict['label'], tf.float32)\n","    # return {'input_ids':token_ids, 'attention_mask':attention_mask, 'token_type_ids':token_type_ids}, label\n","    return {'input_ids':feature_dict['input_ids'], 'attention_mask':feature_dict['attention_mask'], 'token_type_ids':feature_dict['token_type_ids']}, feature_dict['label']\n","\n","# sample_dataset = sample_raw_dataset.map(_read_and_decode)\n","# sample_dataset = sample_dataset.shuffle(buffer_size=1000)\n","# sample_dataset = sample_dataset.batch(batch_size=16)\n","# sample_dataset = sample_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","\n","train_dataset = train_raw_dataset.map(_read_and_decode)\n","train_dataset = train_dataset.shuffle(buffer_size=1000)\n","train_dataset = train_dataset.batch(batch_size=32)\n","train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","valid_dataset = valid_raw_dataset.map(_read_and_decode)\n","valid_dataset = valid_dataset.shuffle(buffer_size=1000)\n","valid_dataset = valid_dataset.batch(batch_size=32)\n","valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQExp2p--fEd","colab_type":"code","colab":{}},"source":["class SPO_Model(TFBertPreTrainedModel):\n","    def __init__(self, config, *inputs, **kwargs):\n","        super().__init__(config, *inputs, **kwargs)\n","        self.bert = TFBertMainLayer(config, name=\"bert\")\n","        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","        # self.BiLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences=True))\n","        self.classifier = tf.keras.layers.Dense(116,activation=tf.nn.sigmoid,kernel_initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05))\n","    def call(self, inputs, **kwargs):\n","        bert_outputs = self.bert(inputs, **kwargs)\n","        sequence_output = bert_outputs[0]\n","        x = self.dropout(sequence_output, training=kwargs.get(\"training\", False))\n","        # x = self.BiLSTM(x)\n","        x = self.classifier(x)\n","        x = tf.keras.layers.Lambda(lambda x: x**(1/0.95))(x) #对BinaryEntropy进行放大处理，以排除人工标注错误\n","        # CRF Layer 其实不适合该模型\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SfSnxt_mAvz","colab_type":"code","colab":{}},"source":["def myloss(y_true, y_pred):\n","\treturn tf.reduce_sum(tf.keras.losses.binary_crossentropy(y_true,y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vuJM5iwjmAv1","colab_type":"code","colab":{}},"source":["spo_model = SPO_Model.from_pretrained(pretrainedModel_path,config=config)\n","\n","spo_model.compile(\n","           optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-5), \n","           loss=myloss, \n","           metrics=[tf.keras.metrics.BinaryAccuracy()]) # accuracy没有好的 随便选了一个"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORAq1ZHWhOQb","colab_type":"code","outputId":"2e26a8bf-12c1-4fd8-f024-6dea8118f8f0","executionInfo":{"status":"ok","timestamp":1587735196839,"user_tz":-480,"elapsed":5168,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":252}},"source":["spo_model.summary()"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Model: \"spo__model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bert (TFBertMainLayer)       multiple                  102267648 \n","_________________________________________________________________\n","dropout_113 (Dropout)        multiple                  0         \n","_________________________________________________________________\n","dense_2 (Dense)              multiple                  89204     \n","=================================================================\n","Total params: 102,356,852\n","Trainable params: 102,356,852\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hWo7udzMxyBB","colab_type":"code","colab":{}},"source":["# with strategy.scope(): # only available data stored in google storage\n","#   spo_model = SPO_Model.from_pretrained(pretrainedModel_path,config=config)\n","#   spo_model.summary()\n","#   spo_model.compile(\n","#            optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-5), \n","#            loss=myloss, \n","#            metrics=[tf.keras.metrics.BinaryAccuracy()]) # accuracy没有好的 随便选了一个"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Alz81-9-13wW","colab_type":"code","outputId":"f621c33e-f8b0-440d-85a0-25ccb360e3c6","executionInfo":{"status":"error","timestamp":1586850787840,"user_tz":-480,"elapsed":62058,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":205}},"source":["spo_model.fit(train_dataset, validation_data=valid_dataset, epochs=5)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","WARNING:tensorflow:Gradients do not exist for variables ['spo__model_2/bert/pooler/dense/kernel:0', 'spo__model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['spo__model_2/bert/pooler/dense/kernel:0', 'spo__model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['spo__model_2/bert/pooler/dense/kernel:0', 'spo__model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['spo__model_2/bert/pooler/dense/kernel:0', 'spo__model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['spo__model_2/bert/pooler/dense/kernel:0', 'spo__model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['spo__model_2/bert/pooler/dense/kernel:0', 'spo__model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['spo__model_2/bert/pooler/dense/kernel:0', 'spo__model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['spo__model_2/bert/pooler/dense/kernel:0', 'spo__model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n"],"name":"stderr"},{"output_type":"stream","text":["    711/Unknown - 1132s 2s/step - loss: 211.8177 - binary_accuracy: 0.9898"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lod2trrfVMzv","colab_type":"code","colab":{}},"source":["spo_model.fit(train_dataset, validation_data=valid_dataset, epochs=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j1L4CGIUxhAi","colab_type":"code","colab":{}},"source":["spo_model.save_weights('drive/ML_Study/saved_Model/weight_200413/spo_model_10epochs.ckpt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eK5M8M8fxizg","colab_type":"code","colab":{}},"source":["spo_model.fit(train_dataset, validation_data=valid_dataset, epochs=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMzM2z1K2ZQw","colab_type":"code","colab":{}},"source":["spo_model.fit(train_dataset, validation_data=valid_dataset, epochs=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSQlE9aJxi_i","colab_type":"code","colab":{}},"source":["spo_model.save_weights('drive/ML_Study/saved_Model/weight_200413v2/spo_model_20epochs.ckpt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-DGCNSCa3Vjl","colab_type":"code","outputId":"43ba1610-5476-44e9-92e9-1b9ad41088fe","executionInfo":{"status":"ok","timestamp":1586575461486,"user_tz":-480,"elapsed":13517,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# spo_model.load_weights('drive/ML_Study/saved_Model/weight_200411/spo_model_10epochs.ckpt')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb7c42dcd68>"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"c37z9kyDbR-E","colab_type":"code","outputId":"5be4f3b3-f189-4620-bb46-51fc6318aab9","executionInfo":{"status":"ok","timestamp":1586574926217,"user_tz":-480,"elapsed":793,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["a = valid_data[3]['text']\n","b = valid_data[3]['spo_list']\n","print(a)\n","print(b)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["这件婚事原本与陈国峻无关，但陈国峻却“欲求配而无由，夜间乃潜入天城公主所居通之\n","[('天城公主', '丈夫_@value', '国峻'), ('国峻', '妻子_@value', '天城公主')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1xDHAq-_bnj4","colab_type":"code","outputId":"127a246c-39b5-4c4e-b047-cf39dc84bb41","executionInfo":{"status":"ok","timestamp":1586574928645,"user_tz":-480,"elapsed":2493,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["a_token = tokenizer.encode(a, max_length=128, pad_to_max_length=True)\n","print(a_token)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[101, 6821, 816, 2042, 752, 1333, 3315, 680, 7357, 1744, 2295, 3187, 1068, 8024, 852, 7357, 1744, 2295, 1316, 100, 3617, 3724, 6981, 5445, 3187, 4507, 8024, 1915, 7313, 718, 4052, 1057, 1921, 1814, 1062, 712, 2792, 2233, 6858, 722, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M8zCjYmbcHUj","colab_type":"code","outputId":"383250ef-30c8-4957-f141-4fd9bd383677","executionInfo":{"status":"ok","timestamp":1586574930471,"user_tz":-480,"elapsed":3061,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":252}},"source":["label = spo_model(tf.constant([a_token]))\n","print(label)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[[6.12396525e-06 9.75153944e-06 2.46941227e-07 ... 9.99919415e-01\n","   1.18960770e-06 1.70931817e-05]\n","  [4.91883213e-07 3.84611167e-06 6.18514449e-08 ... 1.87144406e-06\n","   5.94947691e-09 1.99374242e-08]\n","  [2.18663018e-07 7.74132161e-07 3.71924855e-08 ... 1.19686465e-08\n","   1.55069446e-09 4.32299707e-09]\n","  ...\n","  [2.96188176e-07 2.02725664e-06 2.06809627e-08 ... 7.87636356e-09\n","   2.08752802e-08 1.36730546e-06]\n","  [8.83167900e-07 4.63495053e-06 7.38263211e-08 ... 5.67632652e-09\n","   7.13860047e-08 5.96403243e-06]\n","  [1.10422534e-05 4.13609087e-05 1.59816670e-06 ... 3.09905901e-09\n","   6.09052131e-06 1.65553036e-04]]], shape=(1, 128, 116), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pkQlCjKdb7Q7","colab_type":"code","outputId":"31dfff25-5441-492a-a55f-f6c70a4ff91e","executionInfo":{"status":"ok","timestamp":1586574938059,"user_tz":-480,"elapsed":1051,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rt = predict_SPOes(a_token, label[0])\n","print(rt)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('陈 国 峻', '妻子_@value', '天 城 公 主'), ('天 城 公 主', '丈夫_@value', '陈 国 峻')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O8JPvkV77QoN","colab_type":"code","colab":{}},"source":["# loss, acc = m1.evaluate(input, label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4ApQ4v5tmO_","colab_type":"code","colab":{}},"source":["# label -> SPOes\n","def predict_SPOes(token_id, label): # zip(token_id, label_pred)\n","  SPOes=[]\n","  sub_text_p, sub_category_p = np.where(label[:,:55]>0.75) \n","  obj_text_p, obj_category_p = np.where(label[:,55:110]>0.65) # obj_category_p 自动减去48!! 返回的值是从 0 -> 47!\n","  i_text_p = sorted(np.where(label[:,110]>0.5)) # I 的对应下标\n","  sub_info = list(zip(sub_text_p, sub_category_p))\n","  obj_info = list(zip(obj_text_p, obj_category_p))\n","\n","  SPOes=[]\n","\n","  for sub in sub_info:\n","    sub_start = sub[0]\n","    sub_end = sub_start\n","    for I_i in i_text_p[0]: # 确定 sub 的长度\n","        if I_i == (sub_end + 1):\n","            sub_end += 1\n","    for obj in obj_info: \n","        if sub[1] == (obj[1]):\n","            obj_start = obj[0]\n","            obj_end =  obj_start\n","            for I_i in i_text_p[0]: # 确定 obj 的长度\n","                if I_i == (obj_end + 1):\n","                    obj_end += 1\n","            spo = (\n","                tokenizer.decode(token_id[sub_start: sub_end+1]),\n","                id2predicate[sub[1]],\n","                tokenizer.decode(token_id[obj_start: obj_end+1])\n","                )\n","            SPOes.append(spo)\n","  return SPOes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QS0Di9F_SZmR","colab_type":"code","colab":{}},"source":["def get_relation_pattern(mapping_path):\n","    relation_pattern = {}\n","    with open(mapping_path,encoding='utf-8') as f:\n","        for l in f.readlines():\n","            l = json.loads(l)\n","            relation_pattern[l['predicate']] = (l['object_type'],l['subject_type'])\n","    return relation_pattern\n","\n","relation_pattern = get_relation_pattern(mapping_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MT2aFmgSSXGG","colab_type":"code","colab":{}},"source":["# SPOes -> final output\n","def get_spo_list(SPOes,relation_pattern):\n","    complex_relations = ['配音','上映时间','票房','获奖','饰演']\n","    spo_list= []\n","    for i1, spo in enumerate(SPOes):\n","        p = spo[1].split('_') # 切开predicate\n","        if p[1]  == '@value':\n","            obj={}\n","            obj[p[1]] = spo[2]\n","            obj_type = {}\n","            obj_type[p[1]] = relation_pattern[p[0]][0][p[1]] # object_type\n","            if p[0] in complex_relations: # 如果是复杂关系就找其子预测       \n","                for i2, spo_c in enumerate(SPOes):\n","                    if i1 == i2:\n","                        continue\n","                    p_c = spo_c[1].split('_')\n","                    if p_c[0] == p[0]:\n","                        obj[p_c[1]] = spo_c[2]\n","                        obj_type[p_c[1]] = relation_pattern[p[0]][0][p_c[1]] # object_type\n","        \n","            spo_list.append({\n","                        'predicate':p[0],\n","                        'object_type':obj_type,# 只保留存在的key\n","                        'subject_type':relation_pattern[p[0]][1],\n","                        'object':obj,\n","                        'subject':spo[0]\n","            })\n","    return spo_list"],"execution_count":0,"outputs":[]}]}