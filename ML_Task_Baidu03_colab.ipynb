{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"ML_Task_Baidu03_colab.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rdDYEOMUmAvN","colab_type":"code","outputId":"f8721144-eee0-41dc-f167-88ad3857967b","executionInfo":{"status":"ok","timestamp":1585803008637,"user_tz":-480,"elapsed":147546,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":229}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 144542 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.18-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.18-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.18-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TaU3yOQ5mEQc","colab_type":"code","colab":{}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTNTGW6MmJVV","colab_type":"code","colab":{}},"source":["from drive.ML_Study import * # 让driver连接云盘的ML_Study"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yqr5uYVhmMiI","colab_type":"code","colab":{}},"source":["# For console\n","# 防断连：f12 复制到console里边运行 \n","function ClickConnect(){\n","  console.log(\"Working\"); \n","  document\n","    .querySelector(\"#top-toolbar > colab-connect-button\")\n","    .shadowRoot\n","    .querySelector(\"#connect\")\n","    .click()\n","}\n"," \n","setInterval(ClickConnect,60000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RxTB8oaVmkQI","colab_type":"code","outputId":"c7aedbe8-9411-4376-cce6-d63a3c48784b","executionInfo":{"status":"ok","timestamp":1585803052069,"user_tz":-480,"elapsed":9276,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":700}},"source":["pip install transformers"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl (544kB)\n","\u001b[K     |████████████████████████████████| 552kB 1.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n","\u001b[K     |████████████████████████████████| 870kB 52.2MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 49.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.31)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 51.1MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.31)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (2.8.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=be8241f50dbfb100c80bd5b45a28c3dac805bb9b905ea7837b564565f692b96b\n","  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.7.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RbVOUGI2mEdD","colab_type":"code","colab":{}},"source":["import json\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","import tensorflow as tf\n","from transformers import *\n","import tensorflow_addons as tfa"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZXIiuQzn93S","colab_type":"code","colab":{}},"source":["# input最大长度\n","maxlen=128"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IfklaHtBmAvQ","colab_type":"code","outputId":"604306ba-1349-473d-fdfe-095c7db5e643","executionInfo":{"status":"ok","timestamp":1585803060895,"user_tz":-480,"elapsed":7217,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["print('Lodaing BERT')\n","# 最好使用自己下载的文件，否则可能会因为连不上下载文件的服务器而报错\n","# 而且不是bertGithub的包，使用的是transformers提供的，需要去官方文档那里下载\n","pretrainedModel_path = 'drive/ML_Study/bert-base-chinese'\n","tokenizer = BertTokenizer.from_pretrained(os.path.join(pretrainedModel_path, 'vocab.txt'))\n","config = BertConfig.from_json_file(os.path.join(pretrainedModel_path, 'config.json'))\n","print('==========END==========')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Lodaing BERT\n"],"name":"stdout"},{"output_type":"stream","text":["Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"],"name":"stderr"},{"output_type":"stream","text":["==========END==========\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VqAK1c6DmAvU","colab_type":"code","colab":{}},"source":["# 生成新的SPO\n","def load_data(path):\n","    D = []\n","    with open(path, encoding='utf-8') as f:\n","        for l in f.readlines(): \n","            l = json.loads(l)\n","            D.append({\n","                'text': l['text'],\n","                'spo_list': [\n","                    (spo['subject'], spo['predicate'], spo['object']['@value'])\n","                    for spo in l['spo_list']\n","                ]\n","            })\n","    return D\n","\n","# 得到s,p对应值的初始下标\n","def search(pattern, sequence):\n","    n = len(pattern)\n","    for i in range(len(sequence)):\n","        if sequence[i:i + n] == pattern:\n","            return i\n","    return -1\n","\n","# 寻找特殊符号的位置\n","def search_special_label(pattern, sequence):\n","    rt = []\n","    for i, sid in enumerate(sequence):\n","        if sid == pattern:\n","            rt.append(i)\n","    return rt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"APw3KlkzmAvW","colab_type":"code","outputId":"74149317-48d3-4ef2-ba6e-e2f88383c110","executionInfo":{"status":"ok","timestamp":1585803067000,"user_tz":-480,"elapsed":11659,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print('Loading Data')\n","# train_data = load_data('drive/ML_Study/Baidu_ERE_Data/sample_data.json')\n","train_data = load_data('drive/ML_Study/Baidu_ERE_Data/train_data.json')\n","# valid_data = load_data('drive/ML_Study/B4K_Data/dev_data.json')\n","# test_data = load_data('./data/example.test')\n","print('=========END==========')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Loading Data\n","=========END==========\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aohGFyR7mAvY","colab_type":"code","colab":{}},"source":["# category mapping\n","def get_mapping(mapping_path):\n","    predicate2id, id2predicate = {}, {}\n","    with open(mapping_path, encoding='utf-8') as f:\n","        for l in f.readlines():\n","            l = json.loads(l)\n","            if l['predicate'] not in predicate2id:\n","                id2predicate[len(predicate2id)] = l['predicate']\n","                predicate2id[l['predicate']] = len(predicate2id)\n","    return predicate2id, id2predicate\n","    \n","mapping_path = 'drive/ML_Study/Baidu_ERE_Data/schema.json'\n","\n","predicate2id, id2predicate = get_mapping(mapping_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSeCNf80mAvb","colab_type":"code","colab":{}},"source":["# get BIO-relation label mapping\n","# get predicate mapping\n","special_labels = ['[PAD]','[CLS]','[SEP]','[UNK]']\n","special_labels_token = [0,101,102,100]\n","# All label\n","labels = []\n","for l in predicate2id.keys():\n","    l = 'B-Sub-'+l\n","    labels.append(l)\n","for l in predicate2id.keys():\n","    l = 'B-Obj-'+l\n","    labels.append(l)\n","labels.append('I')\n","labels.append('O')\n","for l in special_labels:\n","    labels.append(l)\n","    \n","# Bilabel Mapping\n","id2BIO = {}\n","BIO2id = {}\n","for i,label in enumerate(labels):\n","    id2BIO[i] = label\n","    BIO2id[label] = i"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yeBjbqlKmAvq","colab_type":"code","colab":{}},"source":["# 这个func应该没用了\n","\n","# 0 1 形式label转为 BIO 形式 \n","# 由于可能一个实体可能既是SUB又是OBJ 而且可能对应多种关系 故采用list的格式存储\n","def visualized_label(label,maxlen=128):\n","    visual_label =[]\n","    for i in range(maxlen):\n","        a=[]\n","        visual_label.append(a)\n","    text_p, category_p= np.where(label>0.5)\n","    for p in zip(text_p, category_p):\n","        visual_label[p[0]].append(id2BIO[p[1]])\n","    return visual_label\n","\n","# print(visualized_abel(label))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFiz0UU2mAvs","colab_type":"code","colab":{}},"source":["# spo 转为 X 和 0 1 形式的 label Y\n","# label: dimensionX->sequencePosition, dimensionY->CategoryPosition\n","def get_input(data,tokenizer,predicate2id,maxlen=128):\n","    special_labels_token = [0,101,102,100]\n","    token_ids, segment_ids, position_ids= [], [], []\n","    spo_labels = []\n","    for d in data:\n","        # get X \n","        token_id, segment_id, position_id = tokenizer.encode_plus(d['text'], max_length=maxlen,pad_to_max_length=True).values()\n","        token_ids.append(token_id)\n","        segment_ids.append(segment_id)\n","        position_ids.append(position_id)\n","        # get Y\n","        # 102 = 2R + 4 + 2 \n","        label = np.zeros((102,maxlen)) # len(labels), maxlen\n","        label[97,:] = 1 # 先设置所有标签都是 O\n","        for s, p, o in d['spo_list']:\n","            s = tokenizer.encode(s)[1:-1]# 101 是 CLS 102 是ESP 这里只是为了得到s内部值的ID\n","            p_i = predicate2id[p] # 对category进行映射得到对应的ID\n","            o = tokenizer.encode(o)[1:-1] \n","            s_i = search(s, token_id) # 对text进行映射\n","            o_i = search(o, token_id)\n","            # 96:'O', 97:'I'\n","            # Sub\n","            label[p_i,s_i] = 1\n","            label[97,s_i] = 0\n","            for i in range(1,len(s)):\n","                s_I_i = s_i + i\n","                label[96,s_I_i] = 1\n","                label[97,s_I_i] = 0\n","            # Obj\n","            label[48+p_i,o_i] = 1\n","            label[97,o_i] = 0\n","            for i in range(1,len(o)):\n","                o_I_i = o_i + i\n","                label[96,o_I_i] = 1\n","                label[97,o_I_i] = 0\n","        # specail label\n","        # 98:'[PAD]'......\n","        for i, sp in enumerate(special_labels_token):\n","            sp_is = search_special_label(sp, token_id)\n","            for sp_i in sp_is:\n","                label[98+i,sp_i] = 1\n","                label[97,sp_i] = 0\n","                \n","        spo_labels.append(label.T)\n","\n","    return [token_ids, position_ids, segment_ids, spo_labels]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTi9uxhPmAvu","colab_type":"code","outputId":"58845c7e-4637-4952-e612-4648649bfd83","executionInfo":{"status":"ok","timestamp":1585803225510,"user_tz":-480,"elapsed":141479,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print('Contructing Data')\n","#这里就可能很大了 可能要先保存为tfrecords再读取 不然不够内存\n","train_xy = get_input(train_data,tokenizer,predicate2id,maxlen=128)\n","print('==========END==========')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Contructing Data\n","==========END==========\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DLPzfa4xJO7n","colab_type":"code","colab":{}},"source":["tfr_data_path = r'drive/ML_Study/Baidu_ERE_Data/TFrecord_data/train.tfrecords'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oQX8-lDxJO--","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":258},"outputId":"900d6e2a-f65c-4102-97c7-b1e27e8ca917","executionInfo":{"status":"error","timestamp":1585801817692,"user_tz":-480,"elapsed":2939,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}}},"source":["# tfrecord 未完成\n","# 改：label 是会报错的 应为二维numpy转不了 example 需要处理\n","with tf.io.TFRecordWriter(tfr_data_path) as writer:\n","    for token_id, position_id, segment_id, spo_label in zip(train_xy[0],train_xy[1],train_xy[2],train_xy[3]):\n","        feature ={\n","            'input_ids' : tf.train.Feature(int64_list=tf.train.Int64List(value=[token_id])),\n","            'attention_mask' : tf.train.Feature(int64_list=tf.train.Int64List(value=[position_id])),\n","            'token_type_ids' : tf.train.Feature(int64_list=tf.train.Int64List(value=[segment_id])),\n","            'label' : tf.train.Feature(float_list=tf.train.FloatList(value=[spo_label]))\n","        }\n","        example = tf.train.Example(features=tf.train.Features(feature=feature))\n","        writer.write(example.SerializeToString())  \n","    writer.close()\n","print('==========END==========')"],"execution_count":41,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-4af6813149c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspo_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_xy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_xy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_xy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_xy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         feature ={\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0;34m'input_ids'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint64_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt64List\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0;34m'attention_mask'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint64_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt64List\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;34m'token_type_ids'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint64_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt64List\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: [101, 1426, 2134, 2150, 6901, 3302, 1218, 4495, 4905, 3184, 3637, 6228, 117, 800, 3698, 1448, 131, 2 has type list, but expected one of: int, long"]}]},{"cell_type":"code","metadata":{"id":"bQExp2p--fEd","colab_type":"code","colab":{}},"source":["class SPO_Model(TFBertPreTrainedModel):\n","    # @property\n","    # def dummy_inputs(self):\n","        # \"\"\" Dummy inputs to build the network.\n","\n","        # Returns:\n","        #     tf.Tensor with dummy inputs\n","        # \"\"\"\n","        # return {\"input_ids\": tf.constant(DUMMY_INPUTS)}\n","\n","    def __init__(self, config, *inputs, **kwargs):\n","        super().__init__(config, *inputs, **kwargs)\n","        self.bert = TFBertMainLayer(config, name=\"bert\")\n","        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","        self.classifier = tf.keras.layers.Dense(102,activation=tf.nn.sigmoid,kernel_initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05))\n","    def call(self, inputs, **kwargs):\n","        bert_outputs = self.bert(inputs, **kwargs)\n","        sequence_output = bert_outputs[0]\n","        x = self.dropout(sequence_output, training=kwargs.get(\"training\", False))\n","        x = self.classifier(x)\n","        # x = tf.keras.layers.Lambda(lambda x: x**(1/0.95))(x) #对BinaryEntropy的放大处理，假设95%的数据都是正确的(排除人工标注错误)\n","        # x = tf.transpose(x,perm=[0,2,1])\n","\n","        # 改： 可以试试加BiLSTM和CRF，或者只加CRF，数据量一大应该有用\n","        # 改： dense的 kernel_initializer 可能要改\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SfSnxt_mAvz","colab_type":"code","colab":{}},"source":["def myloss(y_true, y_pred):\n","\treturn tf.reduce_sum(tf.keras.losses.binary_crossentropy(y_true,y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vuJM5iwjmAv1","colab_type":"code","colab":{}},"source":["spo_model = SPO_Model.from_pretrained(pretrainedModel_path,config=config)\n","\n","spo_model.compile(\n","           optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n","           loss=myloss, \n","           metrics=[tf.keras.metrics.BinaryAccuracy()]) # accuracy没有好的 随便选了一个"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWsaCQNcmAv4","colab_type":"code","outputId":"f7575d34-8b34-46c0-e3bd-b211390bd56a","executionInfo":{"status":"ok","timestamp":1585792076006,"user_tz":-480,"elapsed":50345,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":261}},"source":["spo_model.summary()"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Model: \"spo__model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bert (TFBertMainLayer)       multiple                  102267648 \n","_________________________________________________________________\n","dropout_37 (Dropout)         multiple                  0         \n","_________________________________________________________________\n","dense (Dense)                multiple                  78438     \n","=================================================================\n","Total params: 102,346,086\n","Trainable params: 102,346,086\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_3UcuPXFmAv6","colab_type":"code","outputId":"21afad19-acb6-46b4-ddf8-0693afcc487b","executionInfo":{"status":"ok","timestamp":1585798990642,"user_tz":-480,"elapsed":5107600,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":822}},"source":["spo_model.fit(x={'input_ids':train_xy[0], 'attention_mask':train_xy[1], 'token_type_ids':train_xy[2]},\n","          y=train_xy[3],\n","          epochs=20,\n","          validation_steps=7,\n","          validation_data=([train_xy[0][7000:], train_xy[1][7000:], train_xy[2][7000:]], train_xy[3][7000:]),\n","          batch_size=16\n","           )\n","# 下边的提示是因为我没有使用bert的pool_output\n","\n","# 改：使用TFrecord后试试能不能将batch_size改大？\n","#   然后将epoch调大？但我是数据小才设20，如果数据多了可能会过拟合？试一下！我感觉是lossEntropy<0.5左右就合适了"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","WARNING:tensorflow:Gradients do not exist for variables ['spo__model/bert/pooler/dense/kernel:0', 'spo__model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['spo__model/bert/pooler/dense/kernel:0', 'spo__model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['spo__model/bert/pooler/dense/kernel:0', 'spo__model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['spo__model/bert/pooler/dense/kernel:0', 'spo__model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","626/626 [==============================] - 346s 553ms/step - loss: 102.4228 - binary_accuracy: 0.9920 - val_loss: 31.2979 - val_binary_accuracy: 0.9982\n","Epoch 2/20\n","626/626 [==============================] - 344s 549ms/step - loss: 24.0791 - binary_accuracy: 0.9987 - val_loss: 14.8470 - val_binary_accuracy: 0.9991\n","Epoch 3/20\n","626/626 [==============================] - 344s 549ms/step - loss: 13.4938 - binary_accuracy: 0.9991 - val_loss: 9.5669 - val_binary_accuracy: 0.9992\n","Epoch 4/20\n","626/626 [==============================] - 344s 550ms/step - loss: 9.2691 - binary_accuracy: 0.9992 - val_loss: 6.9558 - val_binary_accuracy: 0.9993\n","Epoch 5/20\n","626/626 [==============================] - 344s 550ms/step - loss: 7.0137 - binary_accuracy: 0.9993 - val_loss: 5.3585 - val_binary_accuracy: 0.9994\n","Epoch 6/20\n","626/626 [==============================] - 345s 551ms/step - loss: 5.5050 - binary_accuracy: 0.9993 - val_loss: 4.2796 - val_binary_accuracy: 0.9994\n","Epoch 7/20\n","626/626 [==============================] - 345s 550ms/step - loss: 4.4650 - binary_accuracy: 0.9994 - val_loss: 3.4280 - val_binary_accuracy: 0.9996\n","Epoch 8/20\n","626/626 [==============================] - 344s 550ms/step - loss: 3.7151 - binary_accuracy: 0.9995 - val_loss: 2.8371 - val_binary_accuracy: 0.9996\n","Epoch 9/20\n","626/626 [==============================] - 344s 550ms/step - loss: 3.0994 - binary_accuracy: 0.9996 - val_loss: 2.4671 - val_binary_accuracy: 0.9996\n","Epoch 10/20\n","626/626 [==============================] - 344s 549ms/step - loss: 2.6905 - binary_accuracy: 0.9996 - val_loss: 1.9643 - val_binary_accuracy: 0.9997\n","Epoch 11/20\n","626/626 [==============================] - 344s 549ms/step - loss: 2.2926 - binary_accuracy: 0.9997 - val_loss: 1.6559 - val_binary_accuracy: 0.9998\n","Epoch 12/20\n","626/626 [==============================] - 344s 549ms/step - loss: 1.9774 - binary_accuracy: 0.9997 - val_loss: 1.5082 - val_binary_accuracy: 0.9998\n","Epoch 13/20\n","626/626 [==============================] - 344s 549ms/step - loss: 1.7255 - binary_accuracy: 0.9997 - val_loss: 1.1052 - val_binary_accuracy: 0.9998\n","Epoch 14/20\n","626/626 [==============================] - 344s 550ms/step - loss: 1.4945 - binary_accuracy: 0.9998 - val_loss: 0.9612 - val_binary_accuracy: 0.9999\n","Epoch 15/20\n","626/626 [==============================] - 344s 550ms/step - loss: 1.3176 - binary_accuracy: 0.9998 - val_loss: 0.8579 - val_binary_accuracy: 0.9999\n","Epoch 16/20\n","626/626 [==============================] - 345s 550ms/step - loss: 1.1803 - binary_accuracy: 0.9998 - val_loss: 0.7256 - val_binary_accuracy: 0.9999\n","Epoch 17/20\n","626/626 [==============================] - 345s 551ms/step - loss: 1.0424 - binary_accuracy: 0.9998 - val_loss: 0.7386 - val_binary_accuracy: 0.9999\n","Epoch 18/20\n","626/626 [==============================] - 345s 551ms/step - loss: 0.9382 - binary_accuracy: 0.9999 - val_loss: 0.5870 - val_binary_accuracy: 0.9999\n","Epoch 19/20\n","626/626 [==============================] - 345s 551ms/step - loss: 0.8548 - binary_accuracy: 0.9999 - val_loss: 0.4881 - val_binary_accuracy: 0.9999\n","Epoch 20/20\n","626/626 [==============================] - 345s 552ms/step - loss: 0.7498 - binary_accuracy: 0.9999 - val_loss: 0.4181 - val_binary_accuracy: 0.9999\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc68c985cf8>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"ak7fCqn_qBFy","colab_type":"code","colab":{}},"source":["# spo_model.save_weights('drive/ML_Study/saved_model/weight_200331/spo_model_200331.ckpt') # 保存权重"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cc461kdN78gK","colab_type":"code","colab":{}},"source":["m1 = SPO_Model.from_pretrained(pretrainedModel_path,config=config)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7SU94p098HHL","colab_type":"code","outputId":"44298f46-a879-481b-b622-9a6f2eb3b872","executionInfo":{"status":"ok","timestamp":1585799724286,"user_tz":-480,"elapsed":14634,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["m1.load_weights('drive/ML_Study/saved_model/weight_200331/spo_model_200331.ckpt') # 加载已经训练的权重"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbc26445048>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"O8JPvkV77QoN","colab_type":"code","colab":{}},"source":["# loss, acc = m1.evaluate(input, label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1Dm9vls1wV_","colab_type":"code","outputId":"c55ebfd2-f6e9-443a-c1d4-ad62567be1f9","executionInfo":{"status":"ok","timestamp":1585799872227,"user_tz":-480,"elapsed":974,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# 测试\n","d = train_data[2]\n","print(d['text'])   \n","print(d['spo_list'])"],"execution_count":32,"outputs":[{"output_type":"stream","text":["贵州顺和建筑工程有限责任公司于2008年1月23日在贵阳市工商行政管理局登记成立\n","[('贵州顺和建筑工程有限责任公司', '成立日期', '2008年1月23日')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ghqKoJAquxss","colab_type":"code","colab":{}},"source":["t_token = tokenizer.encode(d['text'],max_length=128,pad_to_max_length=True)\n","input_token = tf.convert_to_tensor(t_token)[None, :]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9lm3-whB6I1d","colab_type":"code","outputId":"35fbd243-8758-4ea5-afbe-d9b8c26f9810","executionInfo":{"status":"ok","timestamp":1585799875646,"user_tz":-480,"elapsed":966,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":261}},"source":["# rt1 = spo_model.predict([t_token])\n","rt2 = m1(input_token) # 输入必须是tensor\n","# print(rt1[0]) \n","print(rt2[0]) # 直接使用模型和加载保存的模型输出对比 小数点大约后7位开始会有些许不同"],"execution_count":34,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[5.0193185e-06 5.6911274e-05 8.4620897e-06 ... 9.9981052e-01\n","  3.4196801e-05 7.0086178e-05]\n"," [2.7969754e-05 8.3286970e-05 1.6359114e-05 ... 7.7113007e-05\n","  6.5531349e-05 2.2038916e-04]\n"," [2.7397862e-05 1.0262808e-05 7.2903558e-06 ... 1.5741525e-05\n","  1.2611612e-05 1.7541879e-05]\n"," ...\n"," [4.5225247e-06 3.6320348e-06 4.9338355e-06 ... 2.2255656e-06\n","  7.2171883e-06 3.0852075e-06]\n"," [4.5238753e-06 3.6251172e-06 4.8920101e-06 ... 2.2198169e-06\n","  7.4550248e-06 3.1191078e-06]\n"," [4.6151899e-06 3.7761849e-06 4.9036448e-06 ... 2.2287813e-06\n","  7.4868722e-06 3.2095372e-06]], shape=(128, 102), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qPEDS66N2SV1","colab_type":"code","outputId":"0876bc84-c487-4cc8-a766-67a9c1e8f58b","executionInfo":{"status":"ok","timestamp":1585799879317,"user_tz":-480,"elapsed":711,"user":{"displayName":"Kane Huang","photoUrl":"","userId":"02321211776127070843"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["# 01label -> SPO\n","label = rt2[0]\n","\n","token_id, segment_id, position_id = tokenizer.encode_plus(d['text'], max_length=maxlen,pad_to_max_length=True).values()\n","\n","sub_text_p, sub_category_p = np.where(label[:,:48]>0.5) # 可以改为0.6？\n","obj_text_p, obj_category_p = np.where(label[:,48:96]>0.5) # obj_category_p 自动减去48!! 返回的值是从 0 -> 47!\n","i_text_p = sorted(np.where(label[:,96]>0.5)) # I 的对应下标\n","sub_info = list(zip(sub_text_p, sub_category_p))\n","obj_info = list(zip(obj_text_p, obj_category_p))\n","\n","SPOes=[]\n","\n","for sub in sub_info:\n","    sub_start = sub[0]\n","    sub_end = sub_start\n","    for I_i in i_text_p[0]: # 确定 sub 的长度\n","        if I_i == (sub_end + 1):\n","            sub_end += 1\n","    for obj in obj_info: \n","        if sub[1] == (obj[1]):\n","            obj_start = obj[0]\n","            obj_end =  obj_start\n","            for I_i in i_text_p[0]: # 确定 obj 的长度\n","                if I_i == (obj_end + 1):\n","                    obj_end += 1\n","            spo = (\n","                tokenizer.decode(token_id[sub_start: sub_end+1]),\n","                id2predicate[sub[1]],\n","                tokenizer.decode(token_id[obj_start: obj_end+1])\n","                )\n","            SPOes.append(spo)\n","#y_true\n","print(d['text'])\n","print(d['spo_list'])\n","print('='*47)   \n","#y_pred   \n","print(SPOes)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["贵州顺和建筑工程有限责任公司于2008年1月23日在贵阳市工商行政管理局登记成立\n","[('贵州顺和建筑工程有限责任公司', '成立日期', '2008年1月23日')]\n","===============================================\n","[('贵 州 顺 和 建 筑 工 程 有 限 责 任 公 司', '成立日期', '2008 年 1 月 23 日')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i4ApQ4v5tmO_","colab_type":"code","colab":{}},"source":["# label -> SPOes\n","def predict_SPOes(token_id, label): # zip(token_id, label_pred)\n","  SPOes=[]\n","  sub_text_p, sub_category_p = np.where(label[:,:48]>0.5) \n","  obj_text_p, obj_category_p = np.where(label[:,48:96]>0.5) # obj_category_p 自动减去48!! 返回的值是从 0 -> 47!\n","  i_text_p = sorted(np.where(label[:,96]>0.5)) # I 的对应下标\n","  sub_info = list(zip(sub_text_p, sub_category_p))\n","  obj_info = list(zip(obj_text_p, obj_category_p))\n","\n","  SPOes=[]\n","\n","  for sub in sub_info:\n","    sub_start = sub[0]\n","    sub_end = sub_start\n","    for I_i in i_text_p[0]: # 确定 sub 的长度\n","        if I_i == (sub_end + 1):\n","            sub_end += 1\n","    for obj in obj_info: \n","        if sub[1] == (obj[1]):\n","            obj_start = obj[0]\n","            obj_end =  obj_start\n","            for I_i in i_text_p[0]: # 确定 obj 的长度\n","                if I_i == (obj_end + 1):\n","                    obj_end += 1\n","            spo = (\n","                tokenizer.decode(token_id[sub_start: sub_end+1]),\n","                id2predicate[sub[1]],\n","                tokenizer.decode(token_id[obj_start: obj_end+1])\n","                )\n","            SPOes.append(spo)\n","  return SPOes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"87Z_jut5un4T","colab_type":"code","colab":{}},"source":["# 对test集合进行预测并输出比赛结果：\n","spo_M = \n","token_id_test = test_xy[0]\n","# load model \n","spo_M = SPO_Model.from_pretrained(pretrainedModel_path,config=config)\n","spo_M.load_weights('drive/ML_Study/saved_model/weight_200331/spo_model_200331.ckpt')\n","# 还有5个专门针对复杂spo的模型\n","# c_spo_M1 = \n","\n","# output json\n","f = open('dev_pred.json', 'w', encoding='utf-8') \n","# test\n","spo_list_pred_test = []\n","label_pred_test = spo_M(token_id_test)\n","\n","for c in zip(token_id_test, label_pred_test):\n","  SPOes = predict_SPOes(token_id, label)\n","  spo_list_pred_test.append(SPOes)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2vFNkwTl7bno","colab_type":"code","colab":{}},"source":["# 取交集用\n","class SPO(tuple):\n","    def __init__(self, spo):\n","        self.spox = (\n","            tuple(tokenizer.tokenize(spo[0])),\n","            spo[1],\n","            tuple(tokenizer.tokenize(spo[2])),\n","        )\n","\n","    def __hash__(self):\n","        return self.spox.__hash__()\n","\n","    def __eq__(self, spo):\n","        return self.spox == spo.spox"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A_Kyq2lda41r","colab_type":"code","colab":{}},"source":["def evaluate(data):\n","    X, Y, Z = 1e-10, 1e-10, 1e-10\n","    f = open('dev_pred.json', 'w', encoding='utf-8') # 写入\n","    pbar = tqdm()\n","    for d in data:\n","        R = set([SPO(spo) for spo in extract_SPOes(d['text'])]) # pred\n","        T = set([SPO(spo) for spo in d['spo_list']]) # true\n","        X += len(R & T)\n","        Y += len(R)\n","        Z += len(T)\n","        f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n","        pbar.update()\n","        pbar.set_description('f1: %.5f, precision: %.5f, recall: %.5f' %\n","                             (f1, precision, recall))\n","        s = json.dumps(\n","            {\n","                'text': d['text'],\n","                'spo_list': list(T),\n","                'spo_list_pred': list(R),\n","                'new': list(R - T),\n","                'lack': list(T - R),\n","            },\n","            ensure_ascii=False,\n","            indent=4)\n","        f.write(s + '\\n')\n","    pbar.close()\n","    f.close()\n","    return f1, precision, recall"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ERy5IAOa6w_","colab_type":"code","colab":{}},"source":["class Evaluator(tf.keras.callbacks.Callback):\n","    \"\"\"\n","    评估和保存模型\n","    \"\"\"\n","    def __init__(self):\n","        self.best_val_f1 = 0.\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        optimizer.apply_ema_weights()\n","        f1, precision, recall = evaluate(valid_data)\n","        if f1 >= self.best_val_f1:\n","            self.best_val_f1 = f1\n","#             train_model.save_weights('./save/best_model.weights')\n","        optimizer.reset_old_weights()\n","        print('f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n","              (f1, precision, recall, self.best_val_f1))"],"execution_count":0,"outputs":[]}]}